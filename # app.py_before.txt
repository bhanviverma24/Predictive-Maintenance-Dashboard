# app.py
# Predictive Maintenance Dashboard (Streamlit)
# - Comparison: MLP, 1D-CNN, LSTM, Autoencoder
# - Real-time simulation (random synthetic sensor values)
# - Slider-based manual prediction
# - SHAP explainability for MLP

import streamlit as st
st.set_page_config(page_title="PdM Dashboard", layout="wide", initial_sidebar_state="expanded")

import os
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score

import tensorflow as tf
from tensorflow.keras import layers, models

# Optional kagglehub fallback
try:
    import kagglehub
    KAGGLE_AVAILABLE = True
except Exception:
    KAGGLE_AVAILABLE = False

# Optional shap
try:
    import shap
    SHAP_AVAILABLE = True
except Exception:
    SHAP_AVAILABLE = False

# ---------------------------
# Utilities & Caching
# ---------------------------
@st.cache_data(show_spinner=False)
def load_dataset(local_path="data/equipment_anomaly_data.csv"):
    """Load dataset from data/ subfolder; fallback to kaggle if available."""
    if os.path.exists(local_path):
        df = pd.read_csv(local_path)
        source = f"local:{local_path}"
    else:
        if KAGGLE_AVAILABLE:
            path = kagglehub.dataset_download("dnkumars/industrial-equipment-monitoring-dataset")
            # pick first csv
            csvs = [f for f in os.listdir(path) if f.endswith(".csv")]
            if not csvs:
                raise FileNotFoundError("No CSV files found in kaggle dataset folder.")
            df = pd.read_csv(os.path.join(path, csvs[0]))
            source = f"kaggle:{csvs[0]}"
        else:
            raise FileNotFoundError(
                f"{local_path} not found and kagglehub unavailable. Place dataset in data/ folder."
            )
    return df, source

@st.cache_resource(show_spinner=False)
def build_and_train_models(df, target_col='faulty', test_size=0.2, random_state=42, epochs=30):
    """Preprocess dataset and build 4 models. Returns models + test sets + scaler."""
    # select columns
    required_cols = ['temperature', 'vibration']
    if not all(c in df.columns for c in required_cols + [target_col]):
        raise ValueError(f"Dataset must contain columns {required_cols + [target_col]}. Found: {df.columns.tolist()}")

    df = df[required_cols + [target_col]].dropna()
    X = df[required_cols].values.astype(float)
    y = df[target_col].astype(int).values

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # split for base models
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=test_size, random_state=random_state, stratify=y if len(np.unique(y))>1 else None
    )

    # --- MLP (small, fast) ---
    mlp = models.Sequential([
        layers.Input(shape=(2,)),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.25),
        layers.Dense(32, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    mlp.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=0)

    # --- 1D-CNN (light) ---
    X_train_cnn = X_train.reshape((X_train.shape[0], 1, 2))
    X_test_cnn = X_test.reshape((X_test.shape[0], 1, 2))
    cnn = models.Sequential([
        layers.Input(shape=(1,2)),
        layers.Conv1D(32, kernel_size=1, activation='relu'),
        layers.Flatten(),
        layers.Dense(32, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    cnn.fit(X_train_cnn, y_train, epochs=epochs, batch_size=32, verbose=0)

    # --- LSTM ---
    seq_len = 5
    # Build sequence dataset from full scaled X (not just train/test) so LSTM has longer context.
    X_seq, y_seq = [], []
    for i in range(len(X_scaled) - seq_len):
        X_seq.append(X_scaled[i:i+seq_len])
        y_seq.append(y[i+seq_len])
    X_seq = np.array(X_seq)
    y_seq = np.array(y_seq)

    # Handle very small sequences / class imbalance
    if len(X_seq) < 10 or len(np.unique(y_seq)) < 2:
        # fallback: create small synthetic windows by sliding again (safe fallback)
        X_seq = np.stack([X_scaled[max(0,i-seq_len+1):i+1].ravel() for i in range(seq_len, seq_len+10)])
        X_seq = X_seq.reshape((-1, seq_len, 2))
        y_seq = np.repeat(y[:1], X_seq.shape[0])

    X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(
        X_seq, y_seq, test_size=test_size, random_state=random_state, stratify=y_seq if len(np.unique(y_seq))>1 else None
    )
    lstm = models.Sequential([
        layers.Input(shape=(seq_len, 2)),
        layers.LSTM(64),
        layers.Dense(32, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    lstm.fit(X_train_seq, y_train_seq, epochs=epochs, batch_size=32, verbose=0)

    # --- Autoencoder for anomaly detection ---
    X_normal = X_train[y_train == 0] if np.any(y_train == 0) else X_train
    autoencoder = models.Sequential([
        layers.Input(shape=(2,)),
        layers.Dense(16, activation='relu'),
        layers.Dense(8, activation='relu'),
        layers.Dense(16, activation='relu'),
        layers.Dense(2, activation='linear')
    ])
    autoencoder.compile(optimizer='adam', loss='mse')
    autoencoder.fit(X_normal, X_normal, epochs=epochs, batch_size=32, verbose=0)

    # pack outputs
    models_dict = {
        'mlp': mlp,
        'cnn': cnn,
        'lstm': lstm,
        'autoencoder': autoencoder
    }
    data_splits = {
        'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test,
        'X_train_cnn': X_train_cnn, 'X_test_cnn': X_test_cnn,
        'X_train_seq': X_train_seq, 'X_test_seq': X_test_seq, 'y_train_seq': y_train_seq, 'y_test_seq': y_test_seq
    }
    return models_dict, scaler, data_splits

# ---------------------------
# Model utilities
# ---------------------------
def get_predictions(models_dict, data_splits):
    """Compute predictions and metrics for all models."""
    mlp = models_dict['mlp']
    cnn = models_dict['cnn']
    lstm = models_dict['lstm']
    autoencoder = models_dict['autoencoder']

    X_test = data_splits['X_test']
    y_test = data_splits['y_test']
    X_test_cnn = data_splits['X_test_cnn']
    X_test_seq = data_splits['X_test_seq']
    y_test_seq = data_splits['y_test_seq']

    # MLP
    y_mlp_prob = mlp.predict(X_test, verbose=0).ravel()
    y_pred_mlp = (y_mlp_prob > 0.5).astype(int)

    # CNN
    y_cnn_prob = cnn.predict(X_test_cnn, verbose=0).ravel()
    y_pred_cnn = (y_cnn_prob > 0.5).astype(int)

    # LSTM
    y_lstm_prob = lstm.predict(X_test_seq, verbose=0).ravel()
    y_pred_lstm = (y_lstm_prob > 0.5).astype(int)

    # Autoencoder recon error -> threshold
    recon = autoencoder.predict(X_test, verbose=0)
    mse = np.mean(np.square(X_test - recon), axis=1)
    threshold = np.percentile(mse, 95)
    y_pred_auto = (mse > threshold).astype(int)

    metrics = {}
    metrics['mlp'] = {'acc': accuracy_score(y_test, y_pred_mlp),
                      'auc': (roc_auc_score(y_test, y_mlp_prob) if len(np.unique(y_test))>1 else np.nan),
                      'y_true': y_test, 'y_pred': y_pred_mlp}
    metrics['cnn'] = {'acc': accuracy_score(y_test, y_pred_cnn),
                      'auc': (roc_auc_score(y_test, y_cnn_prob) if len(np.unique(y_test))>1 else np.nan),
                      'y_true': y_test, 'y_pred': y_pred_cnn}
    metrics['lstm'] = {'acc': accuracy_score(y_test_seq, y_pred_lstm),
                       'auc': (roc_auc_score(y_test_seq, y_lstm_prob) if len(np.unique(y_test_seq))>1 else np.nan),
                       'y_true': y_test_seq, 'y_pred': y_pred_lstm}
    metrics['auto'] = {'acc': accuracy_score(y_test, y_pred_auto),
                       'auc': (roc_auc_score(y_test, mse) if len(np.unique(y_test))>1 else np.nan),
                       'y_true': y_test, 'y_pred': y_pred_auto}
    return metrics

# --------------------------
# Streamlit Dashboard
# ---------------------------
st.title("Predictive Maintenance Dashboard")
st.markdown("### Deep Learning Comparison for IoT-based Fault Prediction (MLP, CNN, LSTM, Autoencoder)")

menu = st.sidebar.radio(
    "Navigation",
    ["Model Comparison", "Real-Time Simulation", "Manual Prediction", "Model Explainability"]
)

# Load data
try:
    df, src = load_dataset()
    st.sidebar.success(f"Loaded from {src}")
except Exception as e:
    st.sidebar.error(f"Dataset load error: {e}")
    st.stop()

# Train models (cached)
with st.spinner("Training/loading models (this runs once, cached)..."):
    models_dict, scaler, data_splits = build_and_train_models(df, epochs=20)  # reduced epochs for speed
    metrics = get_predictions(models_dict, data_splits)

# -------------------------------------------------------------------------
# 1. MODEL COMPARISON
# -------------------------------------------------------------------------
if menu == "Model Comparison":
    st.subheader("Model Performance Comparison")

    result_df = pd.DataFrame({
        "Model": ["MLP", "1D-CNN", "LSTM", "Autoencoder"],
        "Accuracy": [metrics[m]['acc'] for m in ['mlp', 'cnn', 'lstm', 'auto']],
        "ROC-AUC": [metrics[m]['auc'] for m in ['mlp', 'cnn', 'lstm', 'auto']]
    })

    st.dataframe(result_df.style.highlight_max(axis=0, color="lightgreen"), use_container_width=True)

    fig, ax = plt.subplots(figsize=(6, 4))
    result_df.set_index("Model")[["Accuracy", "ROC-AUC"]].plot(kind="bar", ax=ax, width=0.6)
    plt.title("Model Accuracy and ROC-AUC Comparison", fontsize=13)
    plt.ylabel("Score")
    plt.ylim(0, 1)
    plt.grid(alpha=0.3)
    st.pyplot(fig)

    # Confusion matrices
    st.markdown("#### Confusion Matrices")
    cols = st.columns(2)
    model_names = ['MLP', '1D-CNN', 'LSTM', 'Autoencoder']
    model_keys = ['mlp', 'cnn', 'lstm', 'auto']

    for i, (name, key) in enumerate(zip(model_names, model_keys)):
        y_true = metrics[key]['y_true']
        y_pred = metrics[key]['y_pred']
        # If dimensions mismatch, skip gracefully
        try:
            cm = confusion_matrix(y_true, y_pred)
            fig, ax = plt.subplots(figsize=(3, 3))
            sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False, ax=ax)
            ax.set_title(f"{name} Confusion Matrix")
            ax.set_xlabel("Predicted")
            ax.set_ylabel("Actual")
            cols[i % 2].pyplot(fig)
        except Exception as ex:
            cols[i % 2].warning(f"Cannot plot {name}: {ex}")

# -------------------------------------------------------------------------
# 2. REAL-TIME SIMULATION
# -------------------------------------------------------------------------
elif menu == "Real-Time Simulation":
    st.subheader("Simulated Real-Time Fault Prediction")

    model_map = {"MLP": "mlp", "1D-CNN": "cnn", "Autoencoder": "auto"}
    selected_model = st.selectbox("Select model for simulation:", ["MLP", "1D-CNN", "Autoencoder"])
    model_key = model_map[selected_model]
    model = models_dict[model_key]

    run_sim = st.button("‚ñ∂Ô∏è Start Simulation")
    stop_sim = st.button("‚èπÔ∏è Stop Simulation")
    placeholder = st.empty()

    if run_sim:
        st.info("Generating random sensor readings every second...")
        for _ in range(30):  # run for 30 iterations or until stopped
            temp = np.random.uniform(40, 120)
            vib = np.random.uniform(0.5, 5.0)
            scaled = scaler.transform([[temp, vib]])

            if model_key == "mlp":
                pred = model.predict(scaled, verbose=0)[0][0]
                fault = pred > 0.5
            elif model_key == "cnn":
                pred = model.predict(scaled.reshape(1, 1, 2), verbose=0)[0][0]
                fault = pred > 0.5
            else:  # autoencoder
                recon = model.predict(scaled, verbose=0)[0]
                mse = np.mean(np.square(scaled - recon))
                # Compute threshold from training reconstructions (simple approach):
                # For demo, use percentile of training recon errors (precomputed)
                # Here we set an empirical threshold:
                threshold = 0.05
                fault = mse > threshold

            status = "üö® Fault Detected" if fault else "‚úÖ Normal"
            color = "red" if fault else "green"

            with placeholder.container():
                st.markdown(f"### üå°Ô∏è Temperature: {temp:.2f} ¬∞C  |  ‚öôÔ∏è Vibration: {vib:.2f} m/s¬≤")
                st.markdown(f"<h3 style='color:{color};'>{status}</h3>", unsafe_allow_html=True)
            time.sleep(1)
            # If user clicked stop, break (Streamlit doesn't support direct interruption easily)
            if stop_sim:
                break
    else:
        st.info("Press ‚ñ∂Ô∏è Start Simulation to run a short demo (30s).")

# -------------------------------------------------------------------------
# 3. MANUAL SLIDER PREDICTION
# -------------------------------------------------------------------------
elif menu == "Manual Prediction":
    st.subheader("Manual Input Prediction (Slider)")

    temp = st.slider("Temperature (¬∞C)", 30.0, 120.0, 60.0)
    vib = st.slider("Vibration (m/s¬≤)", 0.2, 5.0, 1.0)

    input_data = np.array([[temp, vib]])
    input_scaled = scaler.transform(input_data)

    model_choice = st.radio("Select model:", ["MLP", "1D-CNN", "Autoencoder"])

    if st.button("üîç Predict"):
        if model_choice == "MLP":
            pred = models_dict["mlp"].predict(input_scaled, verbose=0)[0][0]
            fault = pred > 0.5
        elif model_choice == "1D-CNN":
            pred = models_dict["cnn"].predict(input_scaled.reshape(1, 1, 2), verbose=0)[0][0]
            fault = pred > 0.5
        else:
            recon = models_dict["autoencoder"].predict(input_scaled, verbose=0)[0]
            mse = np.mean(np.square(input_scaled - recon))
            threshold = 0.05
            fault = mse > threshold

        color = "red" if fault else "green"
        status = "‚ö†Ô∏è Machine Fault Detected!" if fault else "‚úÖ Machine Normal"
        st.markdown(f"<h3 style='color:{color};'>{status}</h3>", unsafe_allow_html=True)
        st.write(f"Temperature: {temp:.2f} ¬∞C ‚Äî Vibration: {vib:.2f} m/s¬≤")

# -------------------------------------------------------------------------
# 4. MODEL EXPLAINABILITY
# -------------------------------------------------------------------------
elif menu == "Model Explainability":
    st.subheader("Model Explainability (SHAP for MLP)")
    st.markdown("This explains how temperature and vibration influence the MLP predictions.")

    if not SHAP_AVAILABLE:
        st.warning("SHAP is not installed in this environment. Run `pip install shap` and rerun.")
    else:
        with st.spinner("Computing SHAP values (may take a few seconds)..."):
            # Use a small subset to speed up SHAP
            X_train = data_splits['X_train']
            X_test = data_splits['X_test']
            explainer = shap.Explainer(models_dict['mlp'], X_train[:500])
            shap_values = explainer(X_test[:200])
            plt.figure(figsize=(6,4))
            shap.summary_plot(shap_values, X_test[:200], feature_names=['temperature', 'vibration'], show=False)
            fig = plt.gcf()
            st.pyplot(fig)

# -------------------------------------------------------------------------
# Footer
# -------------------------------------------------------------------------
st.sidebar.markdown("---")
st.sidebar.markdown("Developed for Predictive Maintenance Research ¬© 2025")
